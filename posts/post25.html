<!doctype html>
<html lang="pt-br">
<head>
    <meta charset="UTF-8">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p" crossorigin="anonymous"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="../icons/logoTit.png">
    <link rel="stylesheet" href="../estilo.css">
    <title>UB Social</title>
</head>
<body>
<div class="container-fluid">


    <div class="row">
        <div class="col-sm-12">
            <nav class="navbar rounded-bottom fixed-top navbar-expand-lg navbar-light bg-light shadow">
                <div class="container-fluid">
                    <a class="navbar-brand" href="../index.html"><img src="../icons/logo.png" class="d-inline-block align-text-top" width="11pt"> UB Social</a>
                    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
                    <div class="collapse navbar-collapse" id="navbarNav">
                        <ul class="navbar-nav">
                            <li class="nav-item"><a class="nav-link" href="../index.html">Home</a></li>
                            <li class="nav-item"><a class="nav-link" href="../sobre/sobre.html">Sobre</a></li>
                            <li class="nav-item"><a class="nav-link" href="../cursos.html" id="navCursos">Cursos</a></li>
                            <li class="nav-item"><a class="nav-link" href="../livros/livros.html">Livros</a></li>
                        </ul>
                    </div>
                </div>
            </nav>
        </div>
    </div>

    <div class="row">
        <div class="col-sm-12 text-center" id="titulo">
            <h1>IA qualidade do vinho</h1>
            <h6><strong>Algoritmo ML classificação da qualidade do vinho</strong></h6>
            <a href="../index.html" class="btn btn-link text-decoration-none mb-3">Voltar</a><br>
            <a href="https://youtube.com/playlist?list=PLnPZ9TE1Tj4BcltIriq-kkYgxefpsjCvZ&si=RB5Sj7z5pIzYiJCo" class="btn btn-link text-decoration-none" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" width="1.3em" fill="currentColor" class="bi bi-youtube text-danger" viewBox="0 0 16 16"><path d="M8.051 1.999h.089c.822.003 4.987.033 6.11.335a2.01 2.01 0 0 1 1.415 1.42c.101.38.172.883.22 1.402l.01.104.022.26.008.104c.065.914.073 1.77.074 1.957v.075c-.001.194-.01 1.108-.082 2.06l-.008.105-.009.104c-.05.572-.124 1.14-.235 1.558a2.007 2.007 0 0 1-1.415 1.42c-1.16.312-5.569.334-6.18.335h-.142c-.309 0-1.587-.006-2.927-.052l-.17-.006-.087-.004-.171-.007-.171-.007c-1.11-.049-2.167-.128-2.654-.26a2.007 2.007 0 0 1-1.415-1.419c-.111-.417-.185-.986-.235-1.558L.09 9.82l-.008-.104A31.4 31.4 0 0 1 0 7.68v-.123c.002-.215.01-.958.064-1.778l.007-.103.003-.052.008-.104.022-.26.01-.104c.048-.519.119-1.023.22-1.402a2.007 2.007 0 0 1 1.415-1.42c.487-.13 1.544-.21 2.654-.26l.17-.007.172-.006.086-.003.171-.007A99.788 99.788 0 0 1 7.858 2h.193zM6.4 5.209v4.818l4.157-2.408L6.4 5.209z"/></svg> Conteúdo disponível</a><br>
            <a href="https://www.udemy.com/course/inteligencia-artificial-qualidade-do-vinho" class="btn btn-link text-decoration-none mb-3" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" width="1.3em" fill="currentColor" class="bi bi-book-half text-dark" viewBox="0 0 16 16"><path d="M8.5 2.687c.654-.689 1.782-.886 3.112-.752 1.234.124 2.503.523 3.388.893v9.923c-.918-.35-2.107-.692-3.287-.81-1.094-.111-2.278-.039-3.213.492zM8 1.783C7.015.936 5.587.81 4.287.94c-1.514.153-3.042.672-3.994 1.105A.5.5 0 0 0 0 2.5v11a.5.5 0 0 0 .707.455c.882-.4 2.303-.881 3.68-1.02 1.409-.142 2.59.087 3.223.877a.5.5 0 0 0 .78 0c.633-.79 1.814-1.019 3.222-.877 1.378.139 2.8.62 3.681 1.02A.5.5 0 0 0 16 13.5v-11a.5.5 0 0 0-.293-.455c-.952-.433-2.48-.952-3.994-1.105C10.413.809 8.985.936 8 1.783"/></svg> Curso gratuito na Udemy</a>
            <br>
        </div>

        <div class="col-sm-12">
            <h4>Anexos:</h4>
            <ul>
                <li><a href="https://www.kaggle.com/datasets/uciml/red-wine-quality-cortez-et-al-2009" class="text-decoration-none" target="_blank">Dataset</a></li>
                <li><a href="https://www.kaggle.com/code/nimapourmoradi/red-wine-quality/notebook" class="text-decoration-none" target="_blank">Notebook</a></li>
            </ul>
            
            <br><h4>Importar bibliotecas:</h4>
<small><pre><code>
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
from termcolor import colored

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
print(colored('\nAll libraries imported succesfully.', 'green'))

pd.options.mode.copy_on_write = True #permite reescrever variáveis
sns.set_style('darkgrid') #estilo visual
warnings.filterwarnings('ignore') #ignorar alertas
pd.set_option('display.max_columns', None) #não mostrar todas colunas do dataframe
pd.set_option('display.max_colwidth', None) #não mostrar todos dados do dataframe

sns.color_palette("cool_r", n_colors=1)

sns.set_palette("cool_r")
print(colored('\nAll libraries Configed succesfully.', 'green'))
</code></pre></small>
            <br><h4>Coletar dados:</h4>
<small><pre><code>
#importar dados, via pandas e método de leitura read_csv
data = pd.read_csv('/kaggle/input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')
data.head()
data.info()
data.describe().T.style.background_gradient(axis=0)
data.isna().sum()
</code></pre></small>
            <br><h4>Visualização em gráficos:</h4>
<small><pre><code>
#renomear conlunas para melhor utilização
data.rename(columns = {
    "fixed acidity": "fixed_acidity",
    "volatile acidity": "volatile_acidity",
    "citric acid": "citric_acid",
    "residual sugar": "residual_sugar",
    "chlorides": "chlorides",
    "free sulfur dioxide": "free_sulfur_dioxide",
    "total sulfur dioxide": "total_sulfur_dioxide"},
inplace = True)

columns = list(data.columns) #criar colunas do dataframe

fig, ax = plt.subplots(11, 2, figsize=(15, 45))
plt.subplots_adjust(hspace = 0.5)
for i in range(11):
    sns.boxplot(x=columns[i], data=data, ax=ax[i, 0]) #ax1-gráfico boxplot
    sns.scatterplot(x=columns[i], y='quality', data=data, hue='quality', ax=ax[i, 1]) #ax2-gráfico scatterplot

#não há valores faltantes
#não há outliers (dados que se diferenciam drasticamente de todos os outros)

corr = data.corr()
#cmap = sns.diverging_palette(-1, 1, s=100, l=50, n=15, center="dark", as_cmap=True)
plt.figure(figsize=(9, 6))
sns.heatmap(corr, annot=True, fmt='.2f', linewidth=0.5, cmap='Purples', mask=np.triu(corr)) #gráfico heatmap
plt.show()

sns.pairplot(data, hue='quality', corner = True, palette='Purples') #gráfico pairplot
</code></pre></small>
            <br><h4>Análises Plot:</h4>
            Melhores correlações entre:
            <ul>
                <li>citeic_acid e flex_acidity: 0.67</li>
                <li>density e flex_acidity: 0.67</li>
                <li>total_sulfor_dioxide e free_sulfor_dioxide: 0.67</li>
            </ul>
<small><pre><code>
#categorizações - formação do atributo classe e seus valores
data.quality.unique()
data = data.replace({
    'quality': {
        8: 'Good',
        7: 'Good',
        6: 'Middle',
        5: 'Middle',
        4: 'Bad',
        3: 'Bad',
    }
})
data.head()
</code></pre></small>
            <br><h4>Normalização:</h4>
<small><pre><code>
sns.histplot(data=data, x="fixed_acidity") #gráfico histplot
plt.show() #sem normalização

X_temp = data.drop(columns='quality') #criar colunas de atributos previsores
y = data.quality #criar coluna de atributo classe

scaler = MinMaxScaler(feature_range=(0, 1)).fit_transform(X_temp) #normalizar atributos previsores, para mantê-los entre 0 e 1
X = pd.DataFrame(scaler, columns=X_temp.columns)
X.describe().T.style.background_gradient(axis=0, cmap='Purples')

sns.histplot(data=X, x="fixed_acidity")
plt.show() #com normalização
</code></pre></small>
            <br><h4>Modelagem:</h4>
<small><pre><code>
#função para gerar confusion matrix (dados teste, dados previstos)
def plot_confusion_matrix(y_test, y_prediction):
    cm = metrics.confusion_matrix(y_test, y_prediction)
    ax = plt.subplot()
    ax = sns.heatmap(cm, annot=True, fmt='', cmap="Purples")
    ax.set_xlabel('Prediced labels', fontsize=18)
    ax.set_ylabel('True labels', fontsize=18)
    ax.set_title('Confusion Matrix', fontsize=25)
    ax.xaxis.set_ticklabels(['Bad', 'Good', 'Middle'])
    ax.yaxis.set_ticklabels(['Bad', 'Good', 'Middle'])
    plt.show()

#função para criar classification report
def clfr_plot(y_test, y_pred):
    cr = pd.DataFrame(metrics.classification_report(y_test, y_pred_rf, digits=3, output_dict=True)).T
    cr.drop(columns='support', inplace=True)
    sns.heatmap(cr, cmap='Purples', annot=True, linecolor='white', linewidths=0.5).xaxis.tick_top()

def clf_plot(y_pred):
    #1.criar confusion matrix
    #2.criar classification report
    cm = metrics.confusion_matrix(y_test, y_pred)
    cr = pd.DataFrame(metrics.classification_report(y_test, y_pred, digits=3, output_dict=True)).T
    cr.drop(columns='support', inplace=True)

    fig, ax = plt.subplots(1, 2, figsize=(15, 5))

    #ax esquerdo: confusion matrix
    ax[0] = sns.heatmap(cm, annot=True, fmt='', cmap="Purples", ax=ax[0])
    ax[0].set_xlabel('Prediced labels', fontsize=18)
    ax[0].set_ylabel('True labels', fontsize=18)
    ax[0].set_title('Confusion Matrix', fontsize=25)
    ax[0].xaxis.set_ticklabels(['Bad', 'Good', 'Middle'])
    ax[0].yaxis.set_ticklabels(['Bad', 'Good', 'Middle'])

    #ax direito: classification report
    ax[1] = sns.heatmap(cr, cmap='Purples', annot=True, linecolor='white', linewidths=0.5, ax=ax[1])
    ax[1].xaxis.tick_top()
    ax[1].set_title('Classification Report', fontsize=25)
    plt.show()

data.quality.value_counts()

#dividir dataframe (25% dos dados para teste - geralmente é 80% treino e 20% teste). x são os previsores, y é classe
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)
</code></pre></small>
            <br><h4>Classificação via Random Forest:</h4>
<small><pre><code>
#dicionário de parâmetros para testes no algoritmo
parameters = {
    'n_estimators': [50, 150, 500],
    'criterion': ['gini', 'entropy', 'log_loss'],
    'max_features': ['sqrt', 'log2']
}

rf = RandomForestClassifier(n_jobs=-1)
rf_cv = GridSearchCV(estimator=rf, cv=20, param_grid=parameters).fit(X_train, y_train)

print('Tuned hyper parameters: ', rf_cv.best_params_)
print('accuracy: ', rf_cv.best_score_)

rf = RandomForestClassifier(**rf_cv.best_params_).fit(X_train, y_train) #modelo (treino previsores-classe)

y_pred_rf = rf.predict(X_test)
rf_score = round(rf.score(X_test, y_test), 3) #modelo (teste previsores-classe)
print('RandomForestClassifier score: ', rf_score)

y_test.value_counts()
clf_plot(y_pred_rf)
</code></pre></small>
            <br><h4>Classificação via Logistic Regression:</h4>
<small><pre><code>
parameters = {
    'C': [0.001, 0.01, 0.1, 1.0, 10, 100, 1000],
    'class_weight': ['balanced'],
    'solver': ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga']
}

lr = LogisticRegression()
lr_cv = GridSearchCV(estimator=lr, param_grid=parameters, cv=10).fit(X_train, y_train)

print('Tuned hyper parameters: ', lr_cv.best_params_)
print('accuracy: ', lr_cv.best_score_)

lr = LogisticRegression(**lr_cv.best_params_).fit(X_train, y_train)

y_pred_lr = lr.predict(X_test)
lr_score = round(lr.score(X_test, y_test), 3)
print('LogisticRegression score: ', lr_score)
clf_plot(y_pred_lr)
</code></pre></small>
            <br><h4>Classificação via SVC (Support Vector Classifier):</h4>
<small><pre><code>
parameters = {
    'C': [0.001, 0.01, 0.1, 1.0, 10, 100, 1000],
    'gamma': [0.001, 0.01, 0.1, 1.0, 10, 100, 1000],
}

svc = SVC()
svc_cv = GridSearchCV(estimator=svc, param_grid=parameters, cv=10).fit(X_train, y_train)

print('Tuned hyper parameters: ', svc_cv.best_params_)
print('accuracy: ', svc_cv.best_score_)

svc = SVC(**svc_cv.best_params_).fit(X_train, y_train) #reaplicação do modelo, com melhores parâmetros (treino previsores-classe)

y_pred_svc = svc.predict(X_test)
svc_score = round(svc.score(X_test, y_test), 3)
print('SVC Score: ', svc_score)
clf_plot(y_pred_svc)
</code></pre></small>
            <br><h4>Classificação via Decision Tree:</h4>
<small><pre><code>
parameters = {
    'criterion': ['gini', 'entropy', 'log_loss'],
    'splitter': ['best', 'random'],
    'max_depth': list(np.arange(4, 30, 1))
}

tree = DecisionTreeClassifier()
tree_cv = GridSearchCV(estimator=tree, cv=10, param_grid=parameters).fit(X_train, y_train)

print('Tuned hyper parameters: ', tree_cv.best_params_)
print('accuracy: ', tree_cv.best_score_)

tree = DecisionTreeClassifier(**tree_cv.best_params_).fit(X_train, y_train)

y_pred_tree = tree.predict(X_test)
tree_score = round(tree.score(X_test, y_test), 3)
print('DecisionTreeClassifier Score: ', tree_score)
clf_plot(y_pred_tree)
</code></pre></small>
            <br><h4>Classificação via KNN (k-nearest neighbors):</h4>
<small><pre><code>
parameters = {
    'n_neighbors': list(np.arange(3, 50, 2)),
    'weights': ['uniform', 'distance'],
    'p': [1, 2, 3, 4]
}

knn = KNeighborsClassifier()
knn_cv = GridSearchCV(estimator=knn, cv=10, param_grid=parameters).fit(X_train, y_train)

print('Tuned hyper parameters: ', knn_cv.best_params_)
print('accuracy: ', knn_cv.best_score_)

knn = KNeighborsClassifier(**knn_cv.best_params_).fit(X_train, y_train)

y_pred_knn = knn_cv.predict(X_test)
knn_score = round(knn.score(X_test, y_test), 3)
print('KNeighborsClassifier Score:', knn_score)
clf_plot(y_pred_knn)
</code></pre></small>
            <br><h4>Classificação via Gaussian Naïve Bayes:</h4>
<small><pre><code>
gnb = GaussianNB().fit(X_train, y_train)
y_pred_gnb = gnb.predict(X_test)
gnb_score = round(gnb.score(X_test, y_test), 3)
print('GNB Score:', gnb_score)

clf_plot(y_pred_gnb)
</code></pre></small>
            <br><h4>Conclusões (resultado final):</h4>
<small><pre><code>
result = pd.DataFrame({
    'Algorithm': ['RandomForestClassifier', 'LogisticRegression', 'SVC', 'DecisionTreeClassifier', 'KNeighborsClassifier', 'GaussianNB'],
    'Score': [rf_score, lr_score, svc_score, tree_score, knn_score, gnb_score]
})
result.sort_values(by='Score', inplace=True)
sns.set_palette("Purples")

fig, ax = plt.subplots(1, 1, figsize=(15, 5))
sns.barplot(x='Algorithm', y='Score', data=result)
ax.bar_label(ax.containers[0], fmt='%.3f')
ax.set_xticklabels(labels=result.Algorithm, rotation=300)
plt.show() #SVC possui maior precisão de acerto (score)
</code></pre></small>
            <br><h4>Modelagem final:</h4>
<small><pre><code>
svc = SVC(**svc_cv.best_params_)
svc.fit(X, y) #reaplicação do modelo, com melhores parâmetros (previsores-classe)
</code></pre></small>
        </div>
    </div>


<!--Rodapé-->
<div class="row">
    <div class="col-sm-12 text-center bg-black text-light pt-4 pb-3">
        <p>Elaborado por Mateus Schwede<br><small class="text-muted">ubsocial.github.io</small></p>
    </div>
</div>

</div>
</body>
</html>